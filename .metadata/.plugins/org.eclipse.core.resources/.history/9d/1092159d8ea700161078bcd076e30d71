package Search;


import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;



import tools.Normalizer;

public class WeightsQuery {



	public File corpus;
	public Normalizer normalizer;
	public  File index_; 
	public String query ; 
	 

	public WeightsQuery (Normalizer normalizer, File corpus , File index_ , String query)
	{
		this.normalizer = normalizer; 
		this.corpus = corpus;
		this.index_ = index_;
		this.query = query ;
	}



	
	public HashMap<String,Integer> getTerm () throws IOException
	{	





		HashMap <String , Integer> tf =  new HashMap <String ,Integer> () ; 
		
		ArrayList<String> words = normalizer.normalize(query);
		HashSet<String> wordsInFiles = new HashSet<String>(words) ;
		

		if (corpus.isDirectory()) {

			for (String term : wordsInFiles) {
				term =  term.toLowerCase();
				Integer value =  Collections.frequency(words, term);

				tf.put (term , value) ;

			}

		}
		return tf;
	}
	public HashMap<String,Double> idf () throws IOException
	{	
	HashMap <String , Double> idf =  new HashMap <String ,Double> () ; 
	BufferedReader br = null ;
	br = new BufferedReader(new FileReader(index_));
	String line;
	while ((line = br.readLine()) != null)
	{
		String[] line_parts = line.split("  ");
		String term= line_parts[0];
		
		if(!query.contains(term))
			continue;
		
		String[] docs = line_parts[1].split(",");
		idf.put(term, 1+Math.log(corpus.listFiles().length/docs.length));
		System.out.print(term + idf.get(term));
		}
			
		
	
	
	return idf;
	}
}
