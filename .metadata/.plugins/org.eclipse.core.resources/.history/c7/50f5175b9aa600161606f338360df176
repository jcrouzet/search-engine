package index;





import java.io.*;
import java.math.BigDecimal;
import java.math.RoundingMode;


import java.util.*;

import tools.FrenchTokenizer;
import tools.Normalizer;
public class kkk {


	public File corpus;
	public Normalizer normalizer;
	public File index_; 
	public String query ;

	public kkk (Normalizer normalizer, File corpus , File index_ , String query)
	{
		this.normalizer = normalizer;

		this.corpus = corpus;
		this.index_ = index_;
		this.query = query ;
	}
	
	public void  getIndex() throws IOException
	{	
		if (!index_.exists()) {
			index_.createNewFile();
		}
		else {
			   index_.delete();
			   index_.createNewFile();
		

		FileWriter fw = new FileWriter(index_.getAbsoluteFile());
		BufferedWriter bw = new BufferedWriter(fw);
		
		//  get the occurence of each term in each document
		HashMap<String, HashMap<String, Integer>> tf = new HashMap<String, HashMap<String, Integer>>();
		int NumberOfDocument = 0 ;

		if (corpus.isDirectory()) {
			// get all the files of the directory in order to Calculate of Tf (frequency terms)
			String [] files_corpus = corpus.list();
			ArrayList<String>words = normalizer.normalize(query);	
			for (String file : files_corpus)
			{
		//System.out.print(file);
	//	ArrayList<String>words = normalizer.normalize(query);
		//System.out.print(words);
				//The hash code is then used as the index at which the data associated with the key is stored. 
				//the transformation of the key into its hash code is performed automatically.
				//we use a hashset for no have  2 occurences of the term

				HashSet<String> wordsInFiles = new HashSet<String>(words) ;
			//	System.out.print(wordsInFiles);
				for(String word: wordsInFiles)
				{
					word = word.toLowerCase();
					//  word is null
					if (word==null)
					continue;
						HashMap<String, Integer> value = tf.get(word); 

						if(value == null) 
							value = new HashMap<String, Integer>();
							//The frequency(Collection<?>, Object) method is used to get the number of elements
							//in the specified collection equal to the specified object.
							// example: https://www.tutorialspoint.com/java/util/collections_frequency.htm
							value.put(file, Collections.frequency(words, word));
							tf.put(word,value);
			//	System.out.print(word + "\t" + tf.get(word));
			
		
						
  }
				
			NumberOfDocument ++ ;
			}
		}
	
		for (String word : (new TreeMap<String, HashMap<String, Integer>>(tfs)).keySet())
		{
			HashMap<String, Integer> docs = tfs.get(word);
			HashMap<String, Float> tfidf = new HashMap<String, Float>();
			for(String doc : docs.keySet())
			{
				Integer tf = docs.get(doc);
				Integer df = docs.size();
				Double w = (double)tf * Math.log((double)(9842) / df);
				BigDecimal bd = new BigDecimal(w);
				bd = bd.setScale(2, RoundingMode.HALF_UP);
				tfidf.put(doc, bd.floatValue());
			}
			//tfIdfs.put(word, tfidf);
			bw.println(word + "\t" + tfidf.keySet().toString().replaceAll("[\\[\\] ]", "") + "\t" + tfidf.values().toString().replaceAll("[\\[\\] ]", ""));
		}
		bw.close();
	}
		
	}
			
	public static void main(String[] args) throws IOException {
		 
		
		String StopWords = "/home/anonyme/search-engine/project/data/frenchST.txt";
		File corpus = new File ("/home/anonyme/search-engine/project/data/corpus_reduit");
    		Normalizer tokenizerNoStopWords = new FrenchTokenizer(new File(StopWords));
	     	Normalizer normalizer = tokenizerNoStopWords;
		String path_index = "/home/anonyme/search-engine/project/result_indexation/indlljr";
	     	File index_ = new File(path_index);
			String resquest = "charlie hebdo fatna ";
		    kkk kkk =  new kkk (normalizer ,corpus , index_ , resquest);
	
		kkk.getIndex();
		
		
	
		
	}



}