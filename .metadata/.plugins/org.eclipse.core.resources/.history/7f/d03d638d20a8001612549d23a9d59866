package Search;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;

import tools.FrenchStemmer;
import tools.Normalizer;

public class Rechercher {
	
	

	public File corpus;
	public  Normalizer normalizer;
	public   File index_; 
	public String query ;



	public Rechercher (Normalizer normalizer , String query )
	{
	
		this.query = query ;
		this.normalizer = normalizer;
		
	}
         

	
		ArrayList<String> words = normalizer.normalize(query);
		HashSet<String> wordsInFiles = new HashSet<String>(words);
	
	
ArrayList<HashMap <String,Double >> getDocsWeights (String term)

{
     
	InputStream fis1 = new FileInputStream(index_);
	InputStreamReader isr1 = new InputStreamReader(fis1);
	BufferedReader br1 = new BufferedReader(isr1);
	String line1 = br1.readLine();

	while ( (line1 != null)){
		String[] line_parts = line1.split("  ");
		String term= line_parts[0];
		if(!wordsInFiles.contains(term))
			continue;
			
		String[] Ids =  line_parts[1].split(",");
		String[] Ws = line_parts[2].split(",");
		
	return null;
}

	public static void main(String[] args) throws IOException {
	
		String dir = "/home/anonyme/search-engine/project";
		String StopWords = dir +"/data/frenchST.txt";
	//	File index = new File(dir+"/result_indexation/index_stemmer");
		Normalizer stemmerNoStopWords = new FrenchStemmer(new File(StopWords));
		Normalizer normalizer = stemmerNoStopWords;
		
		String  query =  "charlie hebdcddo charlie ";
		Rechercher ch  = new Rechercher (normalizer , query);
		ch.getTermsQuery();
		
	}
	}

